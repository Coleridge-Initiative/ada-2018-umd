{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, and Jonathan Morgan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "----------\n",
    "Basic dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "    - [Learning Objectives](#Learning-Objectives)\n",
    "    - [Methods](#Methods)\n",
    "- [Python Setup](#Python-Setup)\n",
    "- [Load the Data](#Load-the-Data)\n",
    "    - [Establish a Connection to the Database](#Establish-a-Connection-to-the-Database)\n",
    "    - [Formulate Data Query](#Formulate-Data-Query)\n",
    "    - [Pull Data from the Database](#Pull-Data-from-the-Database)\n",
    "- [Analysis: Using Python and SQL to Analyze Economic Activity in KCMO](#Analysis:-Using-Python-and-SQL-to-Analyze-Economic-Activity-in-KCMO)\n",
    "    - [What is in the Database?](#What-is-in-the-Database?)\n",
    "    - [Summary Statistics on Different Datasets](#Summary-Statistics-on-Different-Datasets)\n",
    "    - [Combining Datasets](#Combining-Datasets)\n",
    "    - [Creating New Measures](#Creating-New-Measures)\n",
    "- [Exercise](#Exercise)\n",
    "- [Submit Results](#Submit-Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In an ideal world, we will have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). \n",
    "However, that is hardly ever true - and we have to work with using our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will discover the datasets we have on the ADRF, and we will use our datasets to answer some questions of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "This notebook will give you the opportunity to spend some hands-on time with the data. \n",
    "\n",
    "You will have an opportunity to explore the different datasets in the ADRF, and this notebook will take you around the different ways you can analyze your data. This involves looking at basic metrics in the larger dataset, taking a random sample, creating derived variables, making sense of the missing values, and so on. \n",
    "\n",
    "This will be done using both SQL and `pandas` in Python. The `sqlalchemy` Python package will give you the opportunity to interact with the database using SQL to pull data into Python. Some additional manipulations will be handled by Pandas in Python (by converting your datasets into dataframes).\n",
    "\n",
    "This notebook will provide an introduction and examples for: \n",
    "\n",
    "- How to create new tables from the larger tables in database (sometimes called the \"analytical frame\")\n",
    "- How to explore different variables of interest\n",
    "- How to explore aggregate metrics\n",
    "- How to handle missing values\n",
    "- How to join newly created tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We will be using the `sqlalchemy` Python package to access tables in our class database server - PostgreSQL. \n",
    "\n",
    "To read the results of our queries, we will be using the `pandas` Python package, which has the ability to read tabular data from SQL queries into a pandas DataFrame object. Within `pandas`, we will use various commands:\n",
    "\n",
    "- Subsetting data\n",
    "- `groupby`\n",
    "- `merge`\n",
    "\n",
    "Within SQL, we will use various queries to:\n",
    "\n",
    "- select data subsets\n",
    "- Sum over groups\n",
    "- create new tables\n",
    "- Count distinct values of desired variables\n",
    "- Order data by chosen variables\n",
    "- Select a random sub-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In Python, we `import` packages. The `import` command allows us to use libraries created by others in our own work by \"importing\" them. You can think of importing a library as opening up a toolbox and pulling out a specific tool. Among the most famous Python packages:\n",
    "- `numpy` is short for \"numerical Python\". `numpy` is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object, and a large suite of functions for doing numerical computing. \n",
    "- `pandas` is a library in Python for data analysis that uses the DataFrame object (modeled after R DataFrames, for those familiar with that language) which is similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click of Excel. It is a lynchpin of the PyData stack and is built on top of `numpy`.  \n",
    "- `sqlalchemy` is a Python library for interfacing with a PostGreSQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# database interaction imports\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When in doubt, use shift + tab to read the documentation of a method.__\n",
    "\n",
    "__The `help()` function provides information on what you can do with a function.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "help(sqlalchemy.create_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We can execute SQL queries using Python to get the best of both worlds. For example, Python - and pandas in particular - make it much easier to calculate descriptive statistics of the data. Additionally, as we will see in the Data Visualization exercises, it is relatively easy to create data visualizations using Python. \n",
    "\n",
    "Pandas provides many ways to load data. It allows the user to read the data from a local csv or excel file, pull the data from a relational database, or read directly from a URL (when you have internet access). Since we are working with the PostgreSQL database `appliedda` in this course, we will demonstrate how to use pandas to read data from a relational database. For examples to read data from a CSV file, refert to the pandas documentation [Getting Data In/Out](pandas.pydata.org/pandas-docs/stable/10min.html#getting-data-in-out).\n",
    "\n",
    "The function to run a SQL query and pull the data into a pandas dataframe (more to come) is `pd.read_sql()`. Just like doing a SQL query from pgAdmin, this function will ask for some information about the database, and what query you would like to run. Let's walk through the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a Connection to the Database\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The first parameter is the connection to the database. To create a connection we will use the SQLAlchemy package and tell it which database we want to connect to, just like in pgAdmin. Additional details on creating a connection to the database are provided in the [Databases](02_1_Databases.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameter 1: Connection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to create a connection to the database, \n",
    "# we need to pass the name of the database and host of the database\n",
    "\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note we can parameterize Python `string` objects - using the built-in `.format()` function. We will use various formulations in the program notebooks (eg when building queries), some examples are:\n",
    "1. Empty brackets (shown above) which simply inserts the variable in the string; when there is more than one set of brackets Python will insert variables in the order they are listed\n",
    "2. Brackets with formatting can be used to make print statements more readable (eg `'text with formatted number with comma and 1-digit decimal {:,.1f}'.format(number_value)` will print `123,456.7` instead of `123456.7123401`)\n",
    "3. Named brackets to use the same variables multiple times in a text block (we use this in more compicated queries eg when creating \"labels\" and \"features\" for Machine Learning models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate Data Query\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "This part is similar to writing a SQL query in pgAdmin. Depending on what data we are interested in, we can use different queries to pull different data. In this example, we will pull all the content of the IL QCEW employers data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create a query as a `string` object in Python__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM il_des_kcmo.il_qcew_employers\n",
    "WHERE year = 2014 AND quarter = 2\n",
    "LIMIT 20\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- the three quotation marks surrounding the query body is called multi-line string. It is quite handy for writing SQL queries because the new line character will be considered part of the string, instead of breaking the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined a variable `query`, we can call it in the code\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the `LIMIT` provides one simple way to get a \"sample\" of data; however, using `LIMIT` does **not provide a _random_** sample. You may get different samples of data than others using just the `LIMIT` clause, but it is just based on what is fastest for the database to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from the Database\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we have the two parameters (database connection and query), we can pass them to the `pd.read_sql()` function, and obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we pass the query and the connection to the pd.read_sql() function and assign the variable `wage` \n",
    "# to the dataframe returned by the function\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Using Python and SQL\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "__What are different measures of employment for TANF recipients?__\n",
    "\n",
    "To explore possible metrics, we will need to combine TANF and employment (in our case, UI wage records) data. We will start slow and explore these datasets individually, then work up to some initial metrics of employment before, during and after receipt of TANF benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in the Database?\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As introduced in the [Databases](./02_1_Databases.ipynb) notebook, there are many ways to connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Schemas, Tables, and Columns in database__\n",
    "\n",
    "Let's pull the list of schema names in the database, the list of tables in these schemas and the list of columns in these tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See all available schemas:\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata;\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As a reminder, in this class you have access to the following schemas: 'public', 'il_des_kcmo', 'il_dhs', 'il_doc_kcmo', 'kcmo_lehd', 'ada_tanf' and your team schema ('ada_tanf_#', where the # is your team number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = \"\"\"\n",
    "'public', 'il_des_kcmo', 'il_dhs', 'il_doc_kcmo', 'kcmo_lehd',  'ada_tanf'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm our schemas exist with \n",
    "# an updated version of the previous query\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata\n",
    "WHERE schema_name IN ({})\n",
    "'''.format(schemas)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT schemaname, tablename\n",
    "FROM pg_tables\n",
    "WHERE schemaname IN ({})\n",
    "'''.format(schemas)\n",
    "\n",
    "tables = pd.read_sql(query, conn)\n",
    "# print tables not in the public schema\n",
    "print(tables.query(\"schemaname != 'public'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the tables in the IL DHS schema:\n",
    "sorted(tables[tables[\"schemaname\"] == 'il_dhs']['tablename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the two ways shown above to subset a `Pandas.DataFrame`:\n",
    "1. Use the built-in `.query()` function\n",
    "2. Create an array of `True` and `False` values (done in this line: `tables[\"schemaname\"] == 'il_dhs'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can look at column names within tables\n",
    "# here we'll set the schema and table with variables\n",
    "\n",
    "schema = 'il_dhs'\n",
    "tbl = 'ind_spells'\n",
    "\n",
    "query = '''\n",
    "SELECT * \n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = '{}' AND table_name = '{}'\n",
    "'''.format(schema, tbl)\n",
    "\n",
    "# read and print results\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Illinois DES datasets__:\n",
    "- `il_des_kcmo.il_qcew_employers`: business level data\n",
    "- `il_des_kcmo.il_wage`: individual job level data\n",
    "- `il_des_kcmo.il_des_establishment`: improved location data for establishements (includes years 2012-2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM il_des_kcmo.il_qcew_employers\n",
    "limit 100;\n",
    "'''\n",
    "il_qcew_employers = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_qcew_employers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, take some time to look at the documentation and understand what the different variables refer to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some employer NAICS codes seem to be missing. Let's see how many records have a value in the `naics` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the SQL query\n",
    "query =\"\"\"\n",
    "SELECT count(*) \n",
    "FROM il_des_kcmo.il_qcew_employers \n",
    "\n",
    "WHERE naics IS NOT NULL\n",
    "\n",
    "AND year = 2009 AND quarter = 2;\n",
    "\"\"\"\n",
    "# read the query into a DataFrame\n",
    "has_naics = pd.read_sql(query, conn)\n",
    "\n",
    "# print the resulting DataFrame\n",
    "has_naics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how we can access the value in our 'has_naics' DataFrame\n",
    "has_naics['count'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What percentage of the records have a NAICS code?\n",
    "\n",
    "# set the SQL query\n",
    "query =\"\"\"\n",
    "SELECT {}./count(*) AS not_missing\n",
    "FROM il_des_kcmo.il_qcew_employers\n",
    "WHERE year = 2009 \n",
    "    AND quarter = 2;\n",
    "\"\"\".format(has_naics['count'][0])\n",
    "\n",
    "# print the query for reference\n",
    "print(query)\n",
    "\n",
    "# read the query and print the result\n",
    "\n",
    "print('{:.1f}% of records have a NAICS code'\\\n",
    ".format(pd.read_sql(query, conn)['not_missing'][0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, some employers are missing their legal name. Let's see how many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is likely that you will see that some employers do not have a legal name. \n",
    "# Let's find how many.\n",
    "\n",
    "#generating read SQL\n",
    "query = '''\n",
    "SELECT count(distinct ein)\n",
    "FROM il_des_kcmo.il_qcew_employers\n",
    "WHERE name_legal is NULL\n",
    "AND year = 2009 AND quarter = 2\n",
    "'''\n",
    "# read the query into a DataFrame\n",
    "missing_names = pd.read_sql(query, conn)\n",
    "# print the resulting DataFrame\n",
    "missing_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Discuss with your team:** what we should do about these missing values?\n",
    "\n",
    "**Exercise:** try coding a similar simple exploration of the `il_des_establishment` record table as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your code...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this section, let's start looking at aggregate statistics on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a few specific, simple questions to better understand our data:\n",
    "- How many jobs are there in our data? \n",
    "- What wages are paid for different types of jobs?\n",
    "- How many TANF cases are there? How many individuals receive TANF?\n",
    "- How many TANF recipients have a job before, during, and after they are enrolled in TANF benefits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: __ Large tables__ can take a long time to process on shared databases. The IL DES UI wage data has over 6M records per quarter, so we will demonstrate using SQL and Python with consideration for how much data we are reading back into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally we'll use the Python time package\n",
    "# to see how long different queries takes to return\n",
    "\n",
    "import time # import time package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example count of records for 2007 Q2 \n",
    "\n",
    "start_time = time.time() # get current time\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM il_des_kcmo.il_wage         \n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(qry, conn)) \n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A **question to consider**: This simple count is one measure of the total jobs in IL in 2007 Q2. What may we want to consider when defining a \"job\" in addition to just being a row in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the easiest way to look at a small subset of records is what\n",
    "# is already demonstrated above: simply add a LIMIT clause\n",
    "# again we'll look at how long this query takes to return\n",
    "\n",
    "# get current time\n",
    "start_time = time.time()\n",
    "\n",
    "query = '''\n",
    "SELECT *\n",
    "FROM il_des_kcmo.il_wage\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "LIMIT 20;\n",
    "'''\n",
    "# get results\n",
    "df_earnings = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earnings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earnings.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reminder: you can refer to the documentation for more information on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get descriptive stats from the DataFrame:\n",
    "df_earnings.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as with the 'count` example, we will get basic stats\n",
    "# from the database using SQL; eg the wage distribution\n",
    "\n",
    "# first, just the 25th percentile value\n",
    "query = \"\"\"\n",
    "SELECT percentile_cont(0.25)\n",
    "    WITHIN GROUP (ORDER BY wage)\n",
    "FROM il_des_kcmo.il_wage\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# display result\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: as of Jan 1, 2016, the minimum wage in IL was \\$8.25 per hour. Assuming a 35 hour work-week and 12 weeks in a quarter, someone working an entire quarter at minumum wage would earn \\$3,465 in the quarter (ignoring taxes). In 2007, IL minimum wage was \\$6.50 per hour, or \\$2,730 per quarter (with the same assumptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of percentile values at which to show the earnings value\n",
    "query = \"\"\"\n",
    "SELECT percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "WITHIN GROUP (ORDER BY wage)\n",
    "FROM il_des_kcmo.il_wage\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# display result\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values above return in a single cell, \n",
    "# add \"unnest\" to get values in a single cell\n",
    "# also add a reference column and column names\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY wage)\n",
    "    ) AS earnings_value,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM il_des_kcmo.il_wage\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# get the result\n",
    "df = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))\n",
    "# view result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to characterize a job is the employer industry. In order to run summary statistics on number of jobs per industry (NAICS code), we need to get the NAICS code from the `il_qcew_employers` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many records from our Earnings data matches the Employers data\n",
    "# just for 2007 Q2 data\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM il_des_kcmo.il_wage AS earn\n",
    "JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "ON earn.ein = empl.ein \n",
    "    AND earn.seinunit = empl.seinunit\n",
    "    AND earn.empr_no = empl.empr_no\n",
    "WHERE earn.year = 2007 AND earn.quarter = 2 \n",
    "    AND empl.year = 2007 AND empl.quarter = 2\n",
    "\"\"\"\n",
    "res = pd.read_sql(qry, conn)\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query time and the PostgreSQL EXPLAIN\n",
    "\n",
    "As you begin creating more advanced or complicated queries, it is useful to have a sense of how long different queries take. One way is to simple record and observe how long different queries take, as has been done in the notebook so far.\n",
    "\n",
    "You may get a sense of when a query is taking a long time. Slow queries could be the result of many people using the database at the same time, the database or table you are using are being `VACUUM`ed, or because of a poorly formed query or data structure (eg not making use of or having `indexes` on columns used to frequently subset or match the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The PostgreSQL \"Explain\" function** can help determine in what order the query is executed and whether it is making use of indexes. It is a little difficult to interpret, but here is an exmaple using the above query. The way to read this is start at the bottom of the RESULT and read up (steps outlined below the query output)\n",
    ">   \n",
    "    -- QUERY:\n",
    "    EXPLAIN \n",
    "    SELECT count(*) \n",
    "    FROM il_des_kcmo.il_wage AS earn\n",
    "    JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "    ON earn.ein = empl.ein AND earn.seinunit = empl.seinunit\n",
    "        AND earn.empr_no = empl.empr_no\n",
    "    WHERE earn.year = 2007 AND earn.quarter = 2 \n",
    "    AND empl.year = 2007 AND empl.quarter = 2\n",
    ">  \n",
    "    -- RESULT\n",
    "    Aggregate  (cost=2733713.11..2733713.12 rows=1 width=0)\n",
    "    ->  Merge Join  (cost=2659260.50..2733713.11 rows=1 width=0)\n",
    "         Merge Cond: ((earn.ein = empl.ein) AND (earn.seinunit = empl.seinunit) AND (earn.empr_no = empl.empr_no))\n",
    "         ->  Sort  (cost=1563641.97..1580871.12 rows=6891659 width=82)\n",
    "               Sort Key: earn.ein, earn.seinunit, earn.empr_no\n",
    "               ->  Append  (cost=0.00..451095.87 rows=6891659 width=82)\n",
    "                     ->  Seq Scan on il_wage earn  (cost=0.00..0.00 rows=1 width=96)\n",
    "                           Filter: ((year = 2007) AND (quarter = 2))\n",
    "                     ->  Seq Scan on il_wage_2007q2 earn_1  (cost=0.00..451095.87 rows=6891658 width=82)\n",
    "                           Filter: ((year = 2007) AND (quarter = 2))\n",
    "         ->  Materialize  (cost=1095618.53..1097832.93 rows=442881 width=80)\n",
    "               ->  Sort  (cost=1095618.53..1096725.73 rows=442881 width=80)\n",
    "                     Sort Key: empl.ein, empl.seinunit, empl.empr_no\n",
    "                     ->  Bitmap Heap Scan on il_qcew_employers empl  (cost=13816.09..1034403.41 rows=442881 width=80)\n",
    "                           Recheck Cond: ((year = 2007) AND (quarter = 2))\n",
    "                           ->  Bitmap Index Scan on il_des_kcmo_qcew_employers_year_quarter_index  (cost=0.00..13705.37 rows=442881 width=0)\n",
    "                                 Index Cond: ((year = 2007) AND (quarter = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this query runs in the following steps:\n",
    "1. scans the employer year and quarter indexes\n",
    "2. returns only rows where year and quarter meet our criteria\n",
    "3. sorts the result based on our 3 identifier columns\n",
    "4. performs same 3 steps on the earnings data (scan, filter, sort)\n",
    "5. merges the two tables based on the 3 identifier columns\n",
    "6. counts the rows (the final, \"Aggregate\" step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, try running explain on a slightly different variant of the previous query:\n",
    "\n",
    "    EXPLAIN SELECT count(*) \n",
    "    FROM il_des_kcmo.il_wage AS earn\n",
    "    JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "    ON earn.ein = empl.ein \n",
    "        AND earn.seinunit = empl.seinunit\n",
    "        AND earn.empr_no = empl.empr_no\n",
    "        AND earn.year = empl.year\n",
    "        AND earn.quarter = empl.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's peruse a sample of a combined dataset\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT earn.ssn, earn.ein, earn.seinunit, earn.empr_no, earn.wage AS earnings,\n",
    "    empl.name_legal, empl.name_trade, multi_unit_code, naics, auxiliary_naics, \n",
    "    total_wages AS tot_wages_paid\n",
    "FROM il_des_kcmo.il_wage AS earn\n",
    "JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "ON earn.ein = empl.ein AND earn.seinunit = empl.seinunit\n",
    "    AND earn.empr_no = empl.empr_no\n",
    "WHERE earn.year = 2007 AND earn.quarter = 2 \n",
    "AND empl.year = 2007 AND empl.quarter = 2\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "res = pd.read_sql(qry, conn)\n",
    "\n",
    "# report how long it took to pull data\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what businesses are in our sample of 100 jobs? and how many jobs are associated with each?\n",
    "res['name_trade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and industries?\n",
    "res['naics'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we see here, the data have 6 digit NAICS codes. We will revisit this when we look at the breakdown of jobs and earnings by industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the distribution of earnings in our sample?\n",
    "res['earnings'].describe(percentiles=[0.1,0.25,0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# and earnings by industry?\n",
    "res.groupby('naics')['earnings'].describe(percentiles=[0.1,0.25,0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll query the database for earnings distribution by 2-digit NAICS in Q2 2007\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT naics2, \n",
    "    unnest(percentile_cont(array[0.1,0.25,0.5, 0.75, 0.9]) \n",
    "    within group (ORDER BY earnings)) AS earnings,\n",
    "    unnest(array[0.1,0.25,0.5, 0.75, 0.9]) AS percentiles\n",
    "FROM (\n",
    "    SELECT earn.wage AS earnings, left(empl.naics, 2) AS naics2\n",
    "    FROM il_des_kcmo.il_wage AS earn\n",
    "    JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "    ON earn.ein = empl.ein AND earn.seinunit = empl.seinunit\n",
    "        AND earn.empr_no = empl.empr_no\n",
    "    WHERE earn.year = 2007 AND earn.quarter = 2 \n",
    "    AND empl.year = 2007 AND empl.quarter = 2\n",
    ") subquery\n",
    "GROUP BY naics2\n",
    "ORDER BY naics2, percentiles\n",
    "\"\"\"\n",
    "res = pd.read_sql(query, conn)\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see from the count of non-null values in the `naics2` column that \n",
    "# there are some missing values in the data - we'll insert an \"Unknown\" flag:\n",
    "res.naics2.fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames also have useful functions like pivot tables:\n",
    "res.pivot_table(values='earnings', columns='percentiles', index='naics2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wages paid by different industry is one way to explore how the **jobs very across industry groups.**\n",
    "\n",
    "## Exploring TANF data\n",
    "\n",
    "Our questions:\n",
    "- How many individuals receive TANF? What are their characteristics?\n",
    "- How many TANF recipients have a job before, during, and after they are enrolled in TANF benefits?\n",
    "\n",
    "Since the TANF data have start and end dates, we will need to consider how our questions relate to dates (whereas with the wage record data we only know if people were paid during a given quarter).\n",
    "\n",
    "Additionally, the TANF data are much more complex so here will focus on just two tables:\n",
    "1. `ind_spells` - individual level spells on different benefits (we'll further focus on just the TANF data, coded as 'tanf46' in this data)\n",
    "2. `member` - includes more information about the people, such as birthdate and gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many spells end in Q4 of 2006?\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT count(*) \n",
    "FROM il_dhs.ind_spells\n",
    "WHERE end_date >= '2006-10-01'::date AND \n",
    "    end_date < '2007-01-01'::date\n",
    "    AND benefit_type = 'tanf46'\n",
    "\"\"\"\n",
    "\n",
    "print('there are {:,.0f} TANF spells'.format(pd.read_sql(query, conn)['count'][0]))\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pull the list's information in the member table\n",
    "# in this query we grab just the recptno values for our\n",
    "# cohort, then use that list to pull the info from \"member\"\n",
    "start_time = time.time()\n",
    "query = \"\"\"\n",
    "SELECT * FROM il_dhs.member\n",
    "WHERE recptno IN ( SELECT recptno\n",
    "    FROM il_dhs.ind_spells\n",
    "    WHERE end_date >= '2006-10-01'::date AND \n",
    "        end_date < '2007-01-01'::date\n",
    "        AND benefit_type = 'tanf46')\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many more results than just our list\n",
    "# does the unique list of recptno match?\n",
    "df['recptno'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unque values for the 'static' variables?\n",
    "static_vars = ['recptno', 'ssn_hash', 'sex', 'rac', 'rootrace', 'foreignbn', 'birth_date']\n",
    "\n",
    "df[static_vars].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(static_vars)['update_id'].count()\\\n",
    ".reset_index()\\\n",
    ".rename(columns={'update_id': 'records'})\\\n",
    ".sort_values('records',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(static_vars)['update_id'].count().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of digging through how the member table is constructed, let's instead base our cohort selection on the `ind_spells` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns from the member table\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ON (i.recptno) i.recptno, i.start_date, i.end_date, \n",
    "    m.birth_date, m.ssn_hash, sex, rac, rootrace\n",
    "FROM il_dhs.ind_spells i\n",
    "LEFT JOIN il_dhs.member m\n",
    "ON i.recptno = m.recptno\n",
    "WHERE end_date >= '2006-10-01'::date AND \n",
    "        end_date < '2007-01-01'::date\n",
    "        AND benefit_type = 'tanf46'\n",
    "\"\"\"\n",
    "# read the data, and additionally parse the dates\n",
    "df = pd.read_sql(query, conn, parse_dates=['start_date', 'end_date', 'birth_date'])\n",
    "print('data read in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of our cohort are there in each of the categories we have?\n",
    "print('count by sex')\n",
    "print(df['sex'].value_counts())\n",
    "print('')\n",
    "print('count by rac')\n",
    "print(df['rac'].value_counts())\n",
    "print('')\n",
    "print('count by rootrace')\n",
    "print(df['rootrace'].value_counts())\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employment and TANF\n",
    "\n",
    "Now we'll explore how many of our cohort were **employed** before, during, or after Q4 of 2006 (note: we'll further need to use their start_date to say if the job was before or during enrollment in this TANF spell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather than selecting the cohort in a sub-query, let's use \n",
    "# the values from the data frame:\n",
    "cohort_ssns = df['ssn_hash'].unique()\n",
    "\n",
    "# reformat the list as a long string of values, this will make it easier to use in the query\n",
    "cohort_ssns = ','.join([\"'\"+ssn+\"'\" for ssn in cohort_ssns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This line of code may look complicated, so let's break it down step by step:\n",
    ">\n",
    "> 1. __`... for ssn in cohort_ssns ...`__ - Loop through every element `ssn` in the list `cohort_ssns`\n",
    "> 2. __`\"'\"+ssn+\"'\" ...`__ - Return SSN value with single quote\n",
    "> 3. __`','.join(...)`__ - join all elements of the list with a comma between them\n",
    ">\n",
    "> _Additional Note: The formulation `[<action> for <item> in <iterable>]`is known as \"list comprehension\"._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# start with before and during\n",
    "# the wage record data starts in 2005 below is only 2 years\n",
    "query = \"\"\"\n",
    "SELECT count(*) recs\n",
    "FROM il_des_kcmo.il_wage\n",
    "WHERE year < 2007\n",
    "    AND ssn IN ({})\n",
    "\"\"\".format(cohort_ssns)\n",
    "\n",
    "print('there are {} records of \"before\" or \"during\" jobs'.format(pd.read_sql(query, conn)['recs'][0]))\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# how many after 2007Q4 - we'll only consider the following 2 years\n",
    "# the wage record data starts in 2005\n",
    "query = \"\"\"\n",
    "SELECT count(*) recs\n",
    "FROM il_des_kcmo.il_wage\n",
    "WHERE year IN (2007, 2008)\n",
    "    AND ssn IN ({})\n",
    "\"\"\".format(cohort_ssns)\n",
    "\n",
    "print('there are {} records of \"after\" jobs'.format(pd.read_sql(query, conn)['recs'][0]))\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pull the \"after\" jobs for some further analysis\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ssn, ein, seinunit, empr_no, wage, year, quarter\n",
    "FROM il_des_kcmo.il_wage\n",
    "WHERE year IN (2007, 2008)\n",
    "    AND ssn IN ({})\n",
    "\"\"\".format(cohort_ssns)\n",
    "df_jobs = pd.read_sql(query, conn)\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many individuals in our cohort had _any_ job in 2007 or 2008?\n",
    "df_jobs['ssn'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.1f}% of our cohort had _a_ job in 2007 and/or 2008'.format(\\\n",
    "100.*df_jobs['ssn'].nunique()/df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Measures\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "> **Questions to consider** (we will revisit similar questions of measurement frequently during the program)\n",
    "0. What problem are we working to solve? How can we measure it?\n",
    "1. How should we define that an individual \"received a job\"? For example, definitions could be \n",
    "  * Received greater than 0 pay at some point within 1 year\n",
    "  * Received greater than minimum wage (assuming XyZ) in 6 of 8 quarters after exiting the TANF program\n",
    "2. How narrowly can you define the unit of analysis? Eg an individual who stopped receiving TANF by...\n",
    "3. What additional information do we know about these individuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary Examples**\n",
    "\n",
    "As the notebooks progress we will dig into different aspects of the above questions, but for now we will show the example of using the `df` dataframe as our study cohort and the `df_jobs` dataframe to create a few example ways to define whether each individual received a job after leaving TANF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example of getting \"any\" job\n",
    "df['ssn_hash'].isin(df_jobs['ssn'].unique()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example of getting \"any\" job - add as column to `df` DataFrame\n",
    "df['emp_any_job'] = df['ssn_hash'].isin(df_jobs['ssn'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least one quarter's earnings are over \"full-time minimum wage\" value of $2,730 in 2007\n",
    "df_jobs[df_jobs['wage']>=2730]['ssn'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_1qtr_overMin'] = df['ssn_hash'].isin(df_jobs[df_jobs['wage']>=2730]['ssn'].unique())\n",
    "df['emp_1qtr_overMin'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least 4 quarters' earnings over \"full-time minimum wage\" value of $2,730 in 2007\n",
    "df_jobs[df_jobs['wage']>=2730].groupby('ssn')['wage'].count().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check records for specific ssn value\n",
    "ssn_val = '...'\n",
    "df_jobs.query(\"ssn == '{}'\".format(ssn_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary dataframe\n",
    "temp_df = df_jobs[df_jobs['wage']>=2730].groupby('ssn')['wage'].count().reset_index().rename(columns={'wage':'count'})\n",
    "\n",
    "# add outcome measure to df\n",
    "df['emp_2qrts_overMin'] = df['ssn_hash'].isin(temp_df[temp_df['count']>=2]['ssn'])\n",
    "df['emp_2qrts_overMin'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_list = ['emp_any_job', 'emp_1qtr_overMin','emp_2qrts_overMin']\n",
    "df[outcome_list].sum() # number of True for each outcome metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separate example: Replicating the QWI Statistics__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [QWI Statistics](../notebooks_additional/03_2_QWI_Statistics.ipynb) notebook demonstrates another example of feature creation: the Quarterly Workforce Indicators Census framework using IL wage records."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "566px",
    "left": "0px",
    "right": "954px",
    "top": "110px",
    "width": "179px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
